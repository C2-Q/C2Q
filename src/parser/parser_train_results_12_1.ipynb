{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-10T21:06:02.874902Z",
     "start_time": "2025-12-10T21:06:02.871232Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:06:04.470397Z",
     "start_time": "2025-12-10T21:06:04.466701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {device}\")"
   ],
   "id": "4f11c30600f651ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:06:07.053308Z",
     "start_time": "2025-12-10T21:06:06.553560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset\n",
    "dataset = load_dataset('csv', data_files={'json': 'data3.csv'})"
   ],
   "id": "97b65ea93a91eb2d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:06:09.332810Z",
     "start_time": "2025-12-10T21:06:08.625634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer, pretrained \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    code_snippet = examples['code_snippet']\n",
    "    return tokenizer(code_snippet, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset['json'].map(tokenize_function, batched=True)\n",
    "\n",
    "# tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ],
   "id": "8b4fb4aa5f610446",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:06:12.477672Z",
     "start_time": "2025-12-10T21:06:11.225812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=10)\n",
    "model.to(device) # move model to mps or cpu\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    # eval_strategy = \"epoch\", \n",
    "    num_train_epochs=10,          # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size (adjust based on memory)\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    logging_strategy=\"steps\",     # Log training loss every few steps\n",
    "    logging_steps=50,             # Log every 50 steps\n",
    "    logging_dir=\"./logs\",         # Auxiliary for tensorboard\n",
    "    save_strategy=\"epoch\",        # Save model after every epoch\n",
    "    load_best_model_at_end=True,  # Load best model when finished training\n",
    "    report_to=\"tensorboard\",             # Report to tensorboard for visualization\n",
    "    \n",
    ")\n",
    "\n",
    "# Split the tokenized dataset into train (80%) and test (20%) sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "# Assign train and test datasets\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n"
   ],
   "id": "a0127078d7eb56ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:12:18.729156Z",
     "start_time": "2025-12-10T21:06:21.086188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train from Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ],
   "id": "84a0e0cb57b5229a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 05:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.877914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.688100</td>\n",
       "      <td>0.282122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.473300</td>\n",
       "      <td>0.132105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.144223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.138844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.114797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.085547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.068472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.069048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.071957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06847164779901505, 'eval_runtime': 1.7396, 'eval_samples_per_second': 50.011, 'eval_steps_per_second': 3.449, 'epoch': 10.0}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### run the following command in terminal for visualization\n",
    "% tensorboard --logdir ./logs"
   ],
   "id": "c8054653745a21af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:54:08.998053Z",
     "start_time": "2025-04-23T09:54:08.995229Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Launch TensorBoard with: tensorboard --logdir ./logs\")",
   "id": "3a2d5213b3d200e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch TensorBoard with: tensorboard --logdir ./logs\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# get attention matrix",
   "id": "55fd20e9491c0d4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:12:27.803823Z",
     "start_time": "2025-12-10T21:12:27.635313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "code_snippet = \"def add(a, b): return a + b\"\n",
    "inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(model.device)\n",
    "# Get attention outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # Attention weights\n",
    "print(attentions)"
   ],
   "id": "b8e4df5715304316",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:16:22.996826Z",
     "start_time": "2025-12-10T21:16:14.590706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Labels for prediction\n",
    "labels = [\"MaxCut\", \"MIS\", \"TSP\", \"Clique\", \"KColor\", \"Factor\", \"ADD\", \"MUL\", \"SUB\", \"VC\"]\n",
    "\n",
    "# Helper: max sequence length for this model\n",
    "try:\n",
    "    MAX_LEN = tokenizer.model_max_length\n",
    "    if MAX_LEN is None or MAX_LEN <= 0 or MAX_LEN > 100000:\n",
    "        # fall back if tokenizer.model_max_length is something weird like 1000000000000\n",
    "        MAX_LEN = model.config.max_position_embeddings\n",
    "except Exception:\n",
    "    MAX_LEN = 512  # safe default\n",
    "\n",
    "def predict(code: str):\n",
    "    # Do NOT send to device yet – first check length\n",
    "    inputs = tokenizer(code, return_tensors=\"pt\")\n",
    "    seq_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    if seq_len > MAX_LEN:\n",
    "        # Raise a clear error; we'll catch it in the loop and report the index\n",
    "        raise ValueError(\n",
    "            f\"Sequence length {seq_len} exceeds model max length {MAX_LEN}\"\n",
    "        )\n",
    "\n",
    "    # Now safe to send to device and run the model\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "    max_prob, prediction = torch.max(probabilities, dim=-1)\n",
    "\n",
    "    return labels[prediction.item()]\n",
    "\n",
    "# Train/test split (you had train_test_split_1 but used train_test_split below)\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, shuffle=False)\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Run prediction on the test set and calculate accuracy manually\n",
    "correct = 0\n",
    "n_too_long = 0\n",
    "too_long_indices = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    code = test_dataset[i]['code_snippet']\n",
    "    true_label = test_dataset[i]['labels']\n",
    "\n",
    "    try:\n",
    "        predicted_label = predict(code)\n",
    "    except ValueError as e:\n",
    "        # Here we report the index that caused the length problem\n",
    "        print(f\"[LONG SEQUENCE] index={i}, error={e}\")\n",
    "        n_too_long += 1\n",
    "        too_long_indices.append(i)\n",
    "        continue  # skip this sample for accuracy\n",
    "\n",
    "    if predicted_label == labels[true_label]:\n",
    "        correct += 1\n",
    "\n",
    "    print(f\"Index {i} | True: {labels[true_label]}, Predicted: {predicted_label}\")\n",
    "\n",
    "# Calculate accuracy over the samples that were actually evaluated\n",
    "evaluated = len(test_dataset) - n_too_long\n",
    "if evaluated > 0:\n",
    "    accuracy = correct / evaluated\n",
    "    print(f\"\\nManual Accuracy (excluding too-long sequences): {accuracy * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nNo sequences were evaluated (all were too long).\")\n",
    "\n",
    "print(f\"Total too-long sequences: {n_too_long}\")\n",
    "print(f\"Indices of too-long sequences: {too_long_indices}\")"
   ],
   "id": "fe7baa4469f25880",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 | True: ADD, Predicted: ADD\n",
      "Index 1 | True: ADD, Predicted: ADD\n",
      "Index 2 | True: SUB, Predicted: SUB\n",
      "Index 3 | True: SUB, Predicted: SUB\n",
      "Index 4 | True: SUB, Predicted: SUB\n",
      "Index 5 | True: SUB, Predicted: SUB\n",
      "Index 6 | True: MUL, Predicted: MUL\n",
      "Index 7 | True: MUL, Predicted: MUL\n",
      "Index 8 | True: MUL, Predicted: MUL\n",
      "Index 9 | True: MUL, Predicted: MUL\n",
      "Index 10 | True: MaxCut, Predicted: MaxCut\n",
      "Index 11 | True: MIS, Predicted: MIS\n",
      "Index 12 | True: Clique, Predicted: Clique\n",
      "Index 13 | True: TSP, Predicted: TSP\n",
      "Index 14 | True: KColor, Predicted: KColor\n",
      "Index 15 | True: KColor, Predicted: KColor\n",
      "Index 16 | True: MIS, Predicted: MIS\n",
      "Index 17 | True: MaxCut, Predicted: MaxCut\n",
      "Index 18 | True: KColor, Predicted: KColor\n",
      "Index 19 | True: ADD, Predicted: ADD\n",
      "Index 20 | True: ADD, Predicted: ADD\n",
      "Index 21 | True: ADD, Predicted: ADD\n",
      "Index 22 | True: ADD, Predicted: ADD\n",
      "Index 23 | True: SUB, Predicted: SUB\n",
      "Index 24 | True: SUB, Predicted: SUB\n",
      "Index 25 | True: SUB, Predicted: SUB\n",
      "Index 26 | True: SUB, Predicted: SUB\n",
      "Index 27 | True: MUL, Predicted: MUL\n",
      "Index 28 | True: MUL, Predicted: MUL\n",
      "Index 29 | True: MUL, Predicted: MUL\n",
      "Index 30 | True: MUL, Predicted: MUL\n",
      "Index 31 | True: MaxCut, Predicted: MaxCut\n",
      "Index 32 | True: MIS, Predicted: MIS\n",
      "Index 33 | True: MIS, Predicted: MIS\n",
      "Index 34 | True: TSP, Predicted: TSP\n",
      "Index 35 | True: TSP, Predicted: TSP\n",
      "Index 36 | True: TSP, Predicted: TSP\n",
      "Index 37 | True: MaxCut, Predicted: MaxCut\n",
      "Index 38 | True: MaxCut, Predicted: MaxCut\n",
      "Index 39 | True: MaxCut, Predicted: MaxCut\n",
      "Index 40 | True: VC, Predicted: VC\n",
      "Index 41 | True: VC, Predicted: VC\n",
      "Index 42 | True: VC, Predicted: VC\n",
      "Index 43 | True: VC, Predicted: VC\n",
      "Index 44 | True: VC, Predicted: VC\n",
      "Index 45 | True: VC, Predicted: VC\n",
      "Index 46 | True: VC, Predicted: VC\n",
      "Index 47 | True: VC, Predicted: VC\n",
      "Index 48 | True: VC, Predicted: VC\n",
      "Index 49 | True: VC, Predicted: VC\n",
      "Index 50 | True: VC, Predicted: VC\n",
      "Index 51 | True: VC, Predicted: VC\n",
      "Index 52 | True: VC, Predicted: VC\n",
      "Index 53 | True: VC, Predicted: VC\n",
      "Index 54 | True: VC, Predicted: VC\n",
      "Index 55 | True: VC, Predicted: VC\n",
      "Index 56 | True: VC, Predicted: VC\n",
      "Index 57 | True: VC, Predicted: VC\n",
      "Index 58 | True: VC, Predicted: VC\n",
      "Index 59 | True: VC, Predicted: VC\n",
      "Index 60 | True: VC, Predicted: VC\n",
      "Index 61 | True: VC, Predicted: VC\n",
      "Index 62 | True: VC, Predicted: VC\n",
      "Index 63 | True: VC, Predicted: VC\n",
      "Index 64 | True: VC, Predicted: VC\n",
      "Index 65 | True: VC, Predicted: VC\n",
      "Index 66 | True: VC, Predicted: VC\n",
      "Index 67 | True: VC, Predicted: VC\n",
      "Index 68 | True: VC, Predicted: VC\n",
      "Index 69 | True: VC, Predicted: VC\n",
      "Index 70 | True: VC, Predicted: VC\n",
      "Index 71 | True: VC, Predicted: VC\n",
      "Index 72 | True: VC, Predicted: VC\n",
      "Index 73 | True: VC, Predicted: VC\n",
      "Index 74 | True: VC, Predicted: VC\n",
      "Index 75 | True: VC, Predicted: VC\n",
      "Index 76 | True: VC, Predicted: VC\n",
      "Index 77 | True: VC, Predicted: VC\n",
      "Index 78 | True: VC, Predicted: VC\n",
      "Index 79 | True: VC, Predicted: VC\n",
      "Index 80 | True: VC, Predicted: VC\n",
      "Index 81 | True: VC, Predicted: VC\n",
      "Index 82 | True: VC, Predicted: VC\n",
      "Index 83 | True: VC, Predicted: VC\n",
      "Index 84 | True: VC, Predicted: VC\n",
      "[LONG SEQUENCE] index=85, error=Sequence length 586 exceeds model max length 512\n",
      "Index 86 | True: VC, Predicted: VC\n",
      "\n",
      "Manual Accuracy (excluding too-long sequences): 100.00%\n",
      "Total too-long sequences: 1\n",
      "Indices of too-long sequences: [85]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:14:38.661282Z",
     "start_time": "2025-12-10T21:13:33.436217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Labels\n",
    "labels = [\"MaxCut\", \"MIS\", \"TSP\", \"Clique\", \"KColor\", \"Factor\", \"ADD\", \"MUL\", \"SUB\", \"VC\"]\n",
    "\n",
    "# Prediction function\n",
    "def predict(code: str):\n",
    "    inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "    _, prediction = torch.max(probabilities, dim=-1)\n",
    "    return prediction.item()\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in tqdm(range(len(tokenized_datasets))):\n",
    "    code = tokenized_datasets[i][\"code_snippet\"]\n",
    "    true_label = tokenized_datasets[i][\"labels\"]\n",
    "    pred_label = predict(code)\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "# Print evaluation report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=3))\n"
   ],
   "id": "5f6788817e290a14",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 434/434 [01:05<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      MaxCut      0.944     1.000     0.971        68\n",
      "         MIS      1.000     0.943     0.971        70\n",
      "         TSP      1.000     1.000     1.000        44\n",
      "      Clique      1.000     1.000     1.000        33\n",
      "      KColor      1.000     1.000     1.000        58\n",
      "      Factor      1.000     1.000     1.000        31\n",
      "         ADD      1.000     1.000     1.000        28\n",
      "         MUL      1.000     1.000     1.000        27\n",
      "         SUB      1.000     1.000     1.000        28\n",
      "          VC      1.000     1.000     1.000        47\n",
      "\n",
      "    accuracy                          0.991       434\n",
      "   macro avg      0.994     0.994     0.994       434\n",
      "weighted avg      0.991     0.991     0.991       434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:27:27.529946Z",
     "start_time": "2025-06-25T12:27:26.604326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save trained model\n",
    "# Save the model and tokenizer locally\n",
    "# Takes some time to store it...\n",
    "model_output_dir = \"./saved_models_2025_12\"\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)\n",
    "print(f\"Model and tokenizer saved to {model_output_dir}\")"
   ],
   "id": "a87c908341b65f52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./saved_models\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# another code cell, manually train",
   "id": "3935269ea1d50c7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T11:08:39.781875Z",
     "start_time": "2024-09-16T11:08:21.217515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'})\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    code_snippet = examples['code_snippet']\n",
    "    return tokenizer(code_snippet, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Specify the fields in the dataset that need to be included\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Define DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(tokenized_datasets['test'], batch_size=8)\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=4)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move input and labels to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Predictions\n",
    "        _, predicted = torch.max(outputs.logits, dim=-1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "id": "7a32e2eec600849a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88912308141c40e3936daac33422fceb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/55 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ada69eb5706414abff326e2dc11dc2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 4 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     52\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask)\n\u001B[0;32m---> 53\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     56\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1188\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1189\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1190\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/torch/nn/functional.py:3104\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3103\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: Target 4 is out of bounds."
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T06:16:25.607205Z",
     "start_time": "2024-09-18T06:16:25.511966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(2, 3), (3, 4), (4, 1)])  # Add multiple edges at onc"
   ],
   "id": "4048e783f244df97",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T06:16:43.689962Z",
     "start_time": "2024-09-18T06:16:43.685737Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "41cd24861d338d23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeView([(2, 3), (3, 4), (4, 1)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T06:43:03.489103Z",
     "start_time": "2024-10-09T06:43:03.444177Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7f018b5a15e3a352",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/numpy/linalg/linalg.py:2180: RuntimeWarning: divide by zero encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/Users/mac/Documents/GitHub/C2Q-Parser/venv/lib/python3.9/site-packages/numpy/linalg/linalg.py:2180: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/var/folders/ys/tg4mzr093dsgjjl04f5n863c0000gn/T/ipykernel_74012/989223126.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plot_bloch_multivector(state).show()\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
